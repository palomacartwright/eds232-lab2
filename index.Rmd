---
title: 'EDS 232: Lab 2 - Clustering'
author: "Paloma Cartwright"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)

librarian::shelf(
  cluster,
  dplyr, 
  DT,
  factoextra,
  ggvoronoi,
  h2o, 
  palmerpenguins,
  tibble, 
  tidyverse,
  vegan,
  scales
)

set.seed(42)
```

# 1. Clustering

This lab was set-up using the `iris` dataset but I will switch the dataset and use `palmerpenguins` instead but with the same applications.  

## 1.1 K-Means Clustering 

k-means clustering specifies the number of clusters needed. The algorithm randomly assigns each observation to a cluster and finds the centroid of each cluster. Then the algorithm: 
- Reassigns data points to the cluster whose centriod is closest
- Calculates the new centriod of each cluster. 

### 1.1.1 Load and plot the `palmerpenguins` dataset 

```{r}
data(penguins) 

if(interactive())
  help(penguins)

datatable(penguins)

penguins <- penguins %>% 
  drop_na()
```

In using `palmerpenguins` instead of `iris`, I will be using `bill_length_mm` instead of `Petal.Length` and `bill_depth_mm` instead of `Petal.Width`. 

```{r}
# plot bill length vs depth, ignoring species 
ggplot(penguins, aes(bill_length_mm, bill_depth_mm)) +
  geom_point()
```

```{r}
#plot bill length vs depth, color by species 

legend_pos <- theme(
  legend.position = c(0.95, 0.05),
  legend.justification = c("right", "bottom"), 
  legend.box.just = "right"
)

ggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm, color = species)) +
  geom_point() +
  legend_pos


```

### 1.1.2 Cluster `penguins` using `kmeans()`

```{r}

k <- 3 # number of clusters
penguins_k <- kmeans(
  penguins %>% 
    select(bill_length_mm, bill_depth_mm),
  centers = k
)

penguins_k
```


```{r}
# compare clusters with species to find out which were not used to cluster 

table(penguins_k$cluster, penguins$species)

```

### **Question**

How many observations could be considered "misclassified" if expecting petal length and width to differentiate between species? 


```{r}
# extract cluster assignment per observation 

Cluster = factor(penguins_k$cluster)

ggplot(penguins, aes(x = bill_length_mm, bill_depth_mm, color = Cluster))+
  geom_point() +
  legend_pos
```

### 1.1.3 Plot Voronoi diagram of clustered `penguins`

#### Bounding Box 

```{r}
# define the bounding box for geom_voronoi()

box <- tribble(
  ~bill_length_mm, ~bill_depth_mm, ~group, 
  30, 12, 1, 
  30, 23, 1, 
  60, 23, 1,
  60, 12, 1, 
  30, 12, 1) %>% 
  data.frame()

```

#### Voronoi for k = 3 

```{r}

#cluster using kmeans()

k <- 3
penguins_k <- kmeans(
  penguins %>% 
    select(bill_length_mm, bill_depth_mm), 
  centers = k)

#extract cluster assignment per observation
Cluster = factor(penguins_k$cluster)

#extract cluster centers 

ctrs <- as.data.frame(penguins_k$centers) %>% 
  mutate(
    Cluster = factor(1:k)
  )

#plot points with voronoi diagram showin the nearest centriod
ggplot(penguins, aes(bill_length_mm, bill_depth_mm, color = Cluster)) +
  geom_point() +
  legend_pos +
  geom_voronoi(
    data = ctrs, 
    aes(fill = Cluster), color = NA, alpha = 0.5, outline = box
  ) + 
  geom_point(
    data = ctrs, pch = 23, cex = 2, fill = "black"
  )


```

#### Voronoi for k = 2

```{r}

#cluster using kmeans()

k <- 2
penguins_k <- kmeans(
  penguins %>% 
    select(bill_length_mm, bill_depth_mm), 
  centers = k)

#extract cluster assignment per observation
Cluster = factor(penguins_k$cluster)

#extract cluster centers 

ctrs <- as.data.frame(penguins_k$centers) %>% 
  mutate(
    Cluster = factor(1:k)
  )

#plot points with voronoi diagram showin the nearest centriod
ggplot(penguins, aes(bill_length_mm, bill_depth_mm, color = Cluster)) +
  geom_point() +
  legend_pos +
  geom_voronoi(
    data = ctrs, 
    aes(fill = Cluster), color = NA, alpha = 0.5, outline = box
  ) + 
  geom_point(
    data = ctrs, pch = 23, cex = 2, fill = "black"
  )


```

#### Voronoi for k = 8 

```{r}

#cluster using kmeans()

k <- 8
penguins_k <- kmeans(
  penguins %>% 
    select(bill_length_mm, bill_depth_mm), 
  centers = k)

#extract cluster assignment per observation
Cluster = factor(penguins_k$cluster)

#extract cluster centers 

ctrs <- as.data.frame(penguins_k$centers) %>% 
  mutate(
    Cluster = factor(1:k)
  )

#plot points with voronoi diagram showin the nearest centriod
ggplot(penguins, aes(bill_length_mm, bill_depth_mm, color = Cluster)) +
  geom_point() +
  legend_pos +
  geom_voronoi(
    data = ctrs, 
    aes(fill = Cluster), color = NA, alpha = 0.5, outline = box
  ) + 
  geom_point(
    data = ctrs, pch = 23, cex = 2, fill = "black"
  )


```


## 1.2 Hierarchical Clustering 

### 1.2.1 Load the `dune` dataset 

```{r}
#load the dune dataset from the vegan package 
data("dune")

if(interactive())
  help(dune)
```

### **Question**

What are the rows and columns composed of in the dune data frame?
The rows in the `dune` data frame are sites and the columns are individual species. 

### 1.2.2 Calculate Ecological Distances on `sites`

```{r}
sites <- tribble(
  ~site, ~sp1, ~sp2, ~sp3,
    "A",    1,    1,    0,
    "B",    5,    5,    0,
    "C",    0,    0,    1) %>% 
  column_to_rownames("site")
sites


sites_manhattan <- vegdist(sites, method = "manhattan")
sites_manhattan

sites_euclidean <- vegdist(sites, method = "euclidean")
sites_euclidean

sites_bray <- vegdist(sites, method = "bray")
sites_bray

```

### 1.2.3 Bray-Curtis Dissimilarity on `sites`

Let's take a closer look at the [Bray-Curtis Dissimilarity](https://en.wikipedia.org/wiki/Bray%E2%80%93Curtis_dissimilarity) distance:

$$
B_{ij} = 1 - \frac{2C_{ij}}{S_i + S_j}
$$

- $B_{ij}$: Bray-Curtis dissimilarity value between sites $i$ and $j$. \
1 = completely dissimilar (no shared species); 0 = identical.

- $C_{ij}$: sum of the lesser counts $C$ for shared species common to both sites $i$ and $j$

- $S_{i OR j}$: sum of all species counts $S$ for the given site $i$ or $j$

So to calculate Bray-Curtis for the example `sites`: 

- $B_{AB} = 1 - \frac{2 * (1 + 1)}{2 + 10} = 1 - 4/12 = 1 - 1/3 = 0.667$

- $B_{AC} = 1 - \frac{2 * 0}{2 + 1} = 1$

- $B_{BC} = 1 - \frac{2 * 0}{10 + 1} = 1$


### 1.2.4 Agglomerative hierarchical clustering on `dune`

```{r}
d <- vegdist(dune, method = "bray")
dim(d)

as.matrix(d)[1:5, 1:5]

# Hierarchical clustering using complete linkage 
hc1 <- hclust(d, method = "complete")

plot(hc1, cex = 0.6, hang = -1)

# Hierarchical clustering using agnes 
hc2 <- agnes(dune, method = "complete")
hc2$ac

plot(hc2, which.plot = 2)


# methods to assess

m <- c("average", "single", "complete", "ward")
names(m) <- c("average", "sinlge", "complete", "ward")

#function to compute coefficient 
ac <- function(x){
  agnes(dune, method = x)$ac
}

#get agglomerative coefficient for each linkage method 
purrr::map_dbl(m, ac)

# Compute ward linkage clustering with agnes
hc3 <- agnes(dune, method = "ward")

hc3$ac

plot(hc3, which.plot = 2)

```

### 1.2.5 Divisive hierarchical clustering on `dune`

```{r}
#compute divisive hierarchical clustering 
hc4 <- diana(dune)

# Divise coefficient; amount of clustering structure found 
hc4$dc

```

### 1.2.6 Determining optimal clusters 

```{r}
p1 <- fviz_nbclust(dune, FUN = hcut, method = "wss", k.max = 10) +
  ggtitle("(A) Elbow method")

p2 <- fviz_nbclust(dune, FUN = hcut, method = "silhouette", k.max = 10) +
  ggtitle("(B) Silhouette method")

p3 <- fviz_nbclust(dune, FUN = hcut, method = "gap_stat", k.max = 10) +
  ggtitle("(C) Gap Statistic")

# display plots side by side 
gridExtra::grid.arrange(p1, p2, p3, nrow = 1)
```


### 1.2.7 Working with dendrograms 

```{r}
# construct dendrogram for the Ames housing example 

hc5 <- hclust(d, method = "ward.D2")
dend_plot <- fviz_dend(hc5)
dend_data <- attr(dend_plot, "dendrogram")
dend_cuts <- cut(dend_data, h = 8)
fviz_dend(dend_cuts$lower[[2]])


# Ward's method 
hc5 <- hclust(d, method = "ward.D2")

# Cut tree into 4 groups 
k = 4
sub_grp <- cutree(hc5, k)

#Number of members in each cluster 
table(sub_grp)


# Plot the full dendogram 
fviz_dend(
  hc5, 
  k = k, 
  horiz = T, 
  rect = T, 
  rect_fill = T, 
  rect_border = "jco", 
  k_colors = "jco"
)

```

# 2. Ordination 

## Principal Components Analysis (PCA)

```{r}
# get data
url <- "https://koalaverse.github.io/homlr/data/my_basket.csv"
my_basket <- readr::read_csv(url)
dim(my_basket)

my_basket
```

## Performing PCA in R 

```{r}
h2o.no_progress()  # turn off progress bars for brevity
h2o.init(max_mem_size = "5g")  # connect to H2O instance

# convert data to h2o object
my_basket.h2o <- as.h2o(my_basket)

# run PCA
my_pca <- h2o.prcomp(
  training_frame = my_basket.h2o,
  pca_method = "GramSVD",
  k = ncol(my_basket.h2o), 
  transform = "STANDARDIZE", 
  impute_missing = TRUE,
  max_runtime_secs = 1000)
my_pca

my_pca@model$eigenvectors %>% 
  as.data.frame() %>% 
  mutate(feature = row.names(.)) %>%
  ggplot(aes(pc1, reorder(feature, pc1))) +
  geom_point()

my_pca@model$eigenvectors %>% 
  as.data.frame() %>% 
  mutate(feature = row.names(.)) %>%
  ggplot(aes(pc1, pc2, label = feature)) +
  geom_text()


```

### Eigenvalue Criterion 

```{r}
# Compute eigenvalues
eigen <- my_pca@model$importance["Standard deviation", ] %>%
  as.vector() %>%
  .^2
  
# Sum of all eigenvalues equals number of variables
sum(eigen)

# Find PCs where the sum of eignvalues is greater than or equal to 1 
which(eigen >= 1)

```




