---
title: 'EDS 232: Lab 2 - Clustering'
author: "Paloma Cartwright"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)

librarian::shelf(
  cluster,
  dplyr, 
  DT,
  factoextra,
  ggvoronoi,
  h2o, 
  palmerpenguins,
  tibble, 
  tidyverse,
  vegan,
  vegan3d, 
  scales, 
  skimr
)

set.seed(42)
```

# 1. Clustering

This lab was set-up using the `iris` dataset but I will switch the dataset and use `palmerpenguins` instead but with the same applications.

## 1.1 K-Means Clustering

k-means clustering specifies the number of clusters needed. The algorithm randomly assigns each observation to a cluster and finds the centroid of each cluster. Then the algorithm: 
- Reassigns data points to the cluster whose centriod is closest 
- Calculates the new centriod of each cluster.

### 1.1.1 Load and plot the `penguins` dataset

```{r}
data(penguins) 

if(interactive())
  help(penguins)

datatable(penguins)

penguins <- penguins %>% 
  drop_na()

skim(penguins)
```

In using `palmerpenguins` instead of `iris`, I will be using `bill_length_mm` instead of `Petal.Length` and `bill_depth_mm` instead of `Petal.Width`.

#### 1 2a.Cluster.geom_point()

```{r}
# plot bill length vs depth, ignoring species 
ggplot(penguins, aes(bill_length_mm, bill_depth_mm)) +
  geom_point()
```

#### 2 2a.Cluster.geom_point()

```{r}
#plot bill length vs depth, color by species 

legend_pos <- theme(
  legend.position = c(0.95, 0.05),
  legend.justification = c("right", "bottom"), 
  legend.box.just = "right"
)

ggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm, color = species)) +
  geom_point() +
  legend_pos


```

### 1.1.2 Cluster `penguins` using `kmeans()`

#### 3 2a.Cluster.kmeans()

```{r}

k <- 3 # number of clusters
penguins_k <- kmeans(
  penguins %>% 
    select(bill_length_mm, bill_depth_mm),
  centers = k
)

penguins_k
```

```{r}
# compare clusters with species to find out which were not used to cluster 

table(penguins_k$cluster, penguins$species)

```

### **Question**

How many observations could be considered "misclassified" if expecting bill length and depth to differentiate between species?

There were 79 observations that could be considered misclassified if expecting bill length and depth to differentiate between species.  


```{r}
# extract cluster assignment per observation 

Cluster = factor(penguins_k$cluster)

ggplot(penguins, aes(x = bill_length_mm, bill_depth_mm, color = Cluster))+
  geom_point() +
  legend_pos
```

#### 4 2a.Cluster.kmeans() **Question**

When comparing the species plot with the `kmeans()` cluster plot, they are mostly similar for bill lengths of up to 41 mm. This means that group two in my cluster plot is very similar to the grouping of Adelie in the species plot. These two graphs differ groups 1 and 3 and the Chinstrap and Gentoo species. The kmeans cluster struggled more with these two groupings than with the Adelie one. 


### 1.1.3 Plot Voronoi diagram of clustered `penguins`

#### Bounding Box

```{r}
# define the bounding box for geom_voronoi()
# define bounding box for geom_voronoi()
xr <- extendrange(range(penguins$bill_length_mm), f=0.1)
yr <- extendrange(range(penguins$bill_depth_mm), f=0.1)
box <- tribble(
  ~bill_length_mm, ~bill_depth_mm, ~group,
  xr[1], yr[1], 1,
  xr[1], yr[2], 1,
  xr[2], yr[2], 1,
  xr[2], yr[1], 1,
  xr[1], yr[1], 1) %>% 
  data.frame()

```

#### Voronoi for k = 3

#### 5 2a.Cluster.geom_voronoi()

```{r}

#cluster using kmeans()

k <- 3
penguins_k <- kmeans(
  penguins %>% 
    select(bill_length_mm, bill_depth_mm), 
  centers = k)

#extract cluster assignment per observation
Cluster = factor(penguins_k$cluster)

#extract cluster centers 

ctrs <- as.data.frame(penguins_k$centers) %>% 
  mutate(
    Cluster = factor(1:k)
  )

#plot points with voronoi diagram showin the nearest centriod
ggplot(penguins, aes(bill_length_mm, bill_depth_mm, color = Cluster)) +
  geom_point() +
  legend_pos +
  geom_voronoi(
    data = ctrs, 
    aes(fill = Cluster), color = NA, alpha = 0.5, outline = box
  ) + 
  geom_point(
    data = ctrs, pch = 23, cex = 2, fill = "black"
  )


```

#### Voronoi for k = 2

```{r}

#cluster using kmeans()

k <- 2
penguins_k <- kmeans(
  penguins %>% 
    select(bill_length_mm, bill_depth_mm), 
  centers = k)

#extract cluster assignment per observation
Cluster = factor(penguins_k$cluster)

#extract cluster centers 

ctrs <- as.data.frame(penguins_k$centers) %>% 
  mutate(
    Cluster = factor(1:k)
  )

#plot points with voronoi diagram showin the nearest centriod
ggplot(penguins, aes(bill_length_mm, bill_depth_mm, color = Cluster)) +
  geom_point() +
  legend_pos +
  geom_voronoi(
    data = ctrs, 
    aes(fill = Cluster), color = NA, alpha = 0.5, outline = box
  ) + 
  geom_point(
    data = ctrs, pch = 23, cex = 2, fill = "black"
  )


```

#### Voronoi for k = 8

```{r}

#cluster using kmeans()

k <- 8
penguins_k <- kmeans(
  penguins %>% 
    select(bill_length_mm, bill_depth_mm), 
  centers = k)

#extract cluster assignment per observation
Cluster = factor(penguins_k$cluster)

#extract cluster centers 

ctrs <- as.data.frame(penguins_k$centers) %>% 
  mutate(
    Cluster = factor(1:k)
  )

#plot points with voronoi diagram showin the nearest centriod
ggplot(penguins, aes(bill_length_mm, bill_depth_mm, color = Cluster)) +
  geom_point() +
  legend_pos +
  geom_voronoi(
    data = ctrs, 
    aes(fill = Cluster), color = NA, alpha = 0.5, outline = box
  ) + 
  geom_point(
    data = ctrs, pch = 23, cex = 2, fill = "black"
  )


```

## 1.2 Hierarchical Clustering

### 1.2.1 Load the `dune` dataset

```{r}
#load the dune dataset from the vegan package 
data("dune")

if(interactive())
  help(dune)
```

### **Question**

What are the rows and columns composed of in the dune data frame? 
The rows in the `dune` data frame are sites and the columns are individual species.

### 1.2.2 Calculate Ecological Distances on `sites`

```{r}
sites <- tribble(
  ~site, ~sp1, ~sp2, ~sp3,
    "A",    1,    1,    0,
    "B",    5,    5,    0,
    "C",    0,    0,    1) %>% 
  column_to_rownames("site")
sites


sites_manhattan <- vegdist(sites, method = "manhattan")
sites_manhattan

sites_euclidean <- vegdist(sites, method = "euclidean")
sites_euclidean

sites_bray <- vegdist(sites, method = "bray")
sites_bray

```

#### 6 2a.Cluster.vegdist() **Question**

The euclidean difference is calculated by measuring the distance between each of the sites. The Bray Curtis distance however is calculated from the differences in the abundance of each species, with a distance of 1 meaning the species are completely different.  

### 1.2.3 Bray-Curtis Dissimilarity on `sites`

Let's take a closer look at the [Bray-Curtis Dissimilarity](https://en.wikipedia.org/wiki/Bray%E2%80%93Curtis_dissimilarity) distance:

$$
B_{ij} = 1 - \frac{2C_{ij}}{S_i + S_j}
$$

-   $B_{ij}$: Bray-Curtis dissimilarity value between sites $i$ and $j$.\
    1 = completely dissimilar (no shared species); 0 = identical.

-   $C_{ij}$: sum of the lesser counts $C$ for shared species common to both sites $i$ and $j$

-   $S_{i OR j}$: sum of all species counts $S$ for the given site $i$ or $j$

So to calculate Bray-Curtis for the example `sites`:

-   $B_{AB} = 1 - \frac{2 * (1 + 1)}{2 + 10} = 1 - 4/12 = 1 - 1/3 = 0.667$

-   $B_{AC} = 1 - \frac{2 * 0}{2 + 1} = 1$

-   $B_{BC} = 1 - \frac{2 * 0}{10 + 1} = 1$

### 1.2.4 Agglomerative hierarchical clustering on `dune`

#### 7 2a.Cluster.as.matrix() 

#### 8 2a.Cluster.hclust()   

```{r}
d <- vegdist(dune, method = "bray")
dim(d)

as.matrix(d)[1:5, 1:5]

# Hierarchical clustering using complete linkage 
hc1 <- hclust(d, method = "complete")

plot(hc1, cex = 0.6, hang = -1)

```

#### 9 2a.Cluster.hclust() **Question**

`vegdist()` comes before `hclust()` because it created the dissimilarity matrix to be plotted in the hierarchical clustering function.

#### 10 2a.Cluster.agnes()

```{r}

# Hierarchical clustering using agnes 
hc2 <- agnes(dune, method = "complete")
hc2$ac

plot(hc2, which.plot = 2)

```

#### 11 2a.Cluster.agnes() **Question**

`hclust()` differs from agnes because it clusters on a set of dissimilarities whereas `agnes()` computes agglomerative hierarchical clustering. 

#### 12 2a.Cluster.ac()

```{r}

# methods to assess

m <- c("average", "single", "complete", "ward")
names(m) <- c("average", "sinlge", "complete", "ward")

#function to compute coefficient 
ac <- function(x){
  agnes(dune, method = x)$ac
}

#get agglomerative coefficient for each linkage method 
purrr::map_dbl(m, ac)
```

#### 13 2a.Cluster.agnes() **Question**

The best model in terms of the agglomerative coefficient is the ward model.

```{r}

# Compute ward linkage clustering with agnes
hc3 <- agnes(dune, method = "ward")

hc3$ac

plot(hc3, which.plot = 2)

```

### 1.2.5 Divisive hierarchical clustering on `dune`

#### 14 2a.Cluster.diana()

```{r}
#compute divisive hierarchical clustering 
hc4 <- diana(dune)

# Divise coefficient; amount of clustering structure found 
hc4$dc

```

#### 15 2a.Cluster.diana()

`agnes()` is agglomerative clustering but `diana()` does not and instead does divisive hierarchical clustering. 

### 1.2.6 Determining optimal clusters

#### 16 2a.Cluster.fviz_nbclust()

```{r}
p1 <- fviz_nbclust(dune, FUN = hcut, method = "wss", k.max = 10) +
  ggtitle("(A) Elbow method")

p2 <- fviz_nbclust(dune, FUN = hcut, method = "silhouette", k.max = 10) +
  ggtitle("(B) Silhouette method")

p3 <- fviz_nbclust(dune, FUN = hcut, method = "gap_stat", k.max = 10) +
  ggtitle("(C) Gap Statistic")

# display plots side by side 
gridExtra::grid.arrange(p1, p2, p3, nrow = 1)
```

#### 17 2a.Cluster.fviz_dend() **Question**

Since A has no dashed line, I compared the other two methods. The Silhouette method gives us an optiimal number of clusters as 4 whereas the Gap Statistic has one less cluster in the optimal number at 3. 

### 1.2.7 Working with dendrograms

```{r}
# construct dendrogram for the Ames housing example 

hc5 <- hclust(d, method = "ward.D2")
dend_plot <- fviz_dend(hc5)
dend_data <- attr(dend_plot, "dendrogram")
dend_cuts <- cut(dend_data, h = 8)
fviz_dend(dend_cuts$lower[[2]])


# Ward's method 
hc5 <- hclust(d, method = "ward.D2")

# Cut tree into 4 groups 
k = 4
sub_grp <- cutree(hc5, k)

#Number of members in each cluster 
table(sub_grp)


# Plot the full dendogram 
fviz_dend(
  hc5, 
  k = k, 
  horiz = T, 
  rect = T, 
  rect_fill = T, 
  rect_border = "jco", 
  k_colors = "jco"
)

```


#### 18 2a.Cluster.fviz_dend() **Question**

In dendrograms, the biggest determinant of relatedness between observations is the height of their shared connection. 


# 2. Ordination

In this lab, you will play with unsupervised classification techniques while working with ecological community datasets.

-   Ordination orders sites near each other based on similarity. It is a multivariate analysis technique used to effectively collapse dependent axes into fewer dimensions, i.e. dimensionality reduction.

    -   Principal Components Analyses (PCA) is the most common and oldest technique that assumes linear relationships between axes. You will follow a non-ecological example from [Chapter 17 Principal Components Analysis](https://bradleyboehmke.github.io/HOML/pca.html) \| Hands-On Machine Learning with R to learn about this commonly used technique.

    -   Non-metric MultiDimensional Scaling (NMDS) allows for non-linear relationships. This ordination technique is implemented in the R package `vegan`. You'll use an ecological dataset, species and environment from lichen pastures that reindeer forage upon, with excerpts from the vegantutor vignette (source) to apply these techniques:

        -   Unconstrained ordination on species using NMDS;
        -   Overlay with environmental gradients; and
        -   Constrained ordination on species and environmnent using another ordination technique, canonical correspondence analysis (CCA).

## Principal Components Analysis (PCA)

```{r}
# get data
url <- "https://koalaverse.github.io/homlr/data/my_basket.csv"
my_basket <- readr::read_csv(url)
dim(my_basket)

my_basket
```

## Performing PCA in R

#### 19 2b.Community.h2o.prcomp()

```{r}
h2o.no_progress()  # turn off progress bars for brevity
h2o.init(max_mem_size = "5g")  # connect to H2O instance

# convert data to h2o object
my_basket.h2o <- as.h2o(my_basket)

ncol(my_basket.h2o)

# run PCA
my_pca <- h2o.prcomp(
  training_frame = my_basket.h2o,
  pca_method = "GramSVD",
  k = ncol(my_basket.h2o), 
  transform = "STANDARDIZE", 
  impute_missing = TRUE,
  max_runtime_secs = 1000)
my_pca

```

#### 20 2b.Community.h2o.prcomp() **Question**

"GramSVD" is used over the "GLRM" method because the data is mostly numeric and GLRM is better for categorical variables. 

#### 21 2b.Community.h2o.prcomp() **Question**

There were 42 initial principal components chosen which was the number of columns in the input data. 

#### 22 2b.Community.geom_point()

```{r}

my_pca@model$eigenvectors %>% 
  as.data.frame() %>% 
  mutate(feature = row.names(.)) %>%
  ggplot(aes(pc1, reorder(feature, pc1))) +
  geom_point()

```

#### 23 2b.Community.geom_point() **Question**

Adult beverages like alcohol contribute the most to PC1.

#### 24. 2b.Community.geom_text() 

```{r}

my_pca@model$eigenvectors %>% 
  as.data.frame() %>% 
  mutate(feature = row.names(.)) %>%
  ggplot(aes(pc1, pc2, label = feature)) +
  geom_text()


```

#### 25 2b.Community.geom_text() **Question**

Milk, tea, instant coffee and muesli, breakfast items contribute the most to pc2 and least to pc1. The veggies cluster can also be said to contribute the least to pc1 and most to pc2. 


### Eigenvalue Criterion

#### 26 2b.Community.facet_wrap()

```{r}
# Compute eigenvalues
eigen <- my_pca@model$importance["Standard deviation", ] %>%
  as.vector() %>%
  .^2
  
# Sum of all eigenvalues equals number of variables
sum(eigen)

# Find PCs where the sum of eignvalues is greater than or equal to 1 
which(eigen >= 1)

# Extract PVE nad CVE 

ve <- data.frame(
  PC = my_pca@model$importance %>% seq_along(),
  PVE = my_pca@model$importance %>% .[2,] %>% unlist(),
  CVE = my_pca@model$importance %>% .[3,] %>% unlist()
)

# Plot PVE and CVE 

ve %>% 
  tidyr::gather(metric, variance_explained, -PC) %>% 
  ggplot(aes(PC, variance_explained)) +
  geom_point() +
  facet_wrap(~ metric, ncol = 1, scales = "free")

```

#### 27 2b.Community.@model$importance 

I would include about 35 principal components to explain 90% of the total variance. 


#### 28 2b.Community.geom_line() 

```{r}
# How many PCs required to explain at least 75% of total variability
min(which(ve$CVE >= 0.75))

# Screen plot criterion 
data.frame(
  PC = my_pca@model$importance %>% seq_along,
  PVE = my_pca@model$importance %>% .[2,] %>% unlist()) %>% 
  ggplot(aes(PC, PVE, group = 1, label = PC)) +
  geom_point()+
  geom_line() +
  geom_text(nudge_y = -.002)

```

#### 29 2b.Community.geom_line() **Question**

There are 6 principal components to include before the elbow. 

#### 30 2b.Community.PCA **Question**

PCA can be affected by outliers and it does not perform well where complex non-linear patterns often exist. 


## Non-metric MultiDimernsional Scaling (NMDS)

### Unconstrained Ordination on Species

```{r}
# vegetation and environment in lichen pastures from Vare et al (1995) 

data("varespec") # species
data("varechem") # chemistry 

varespec %>% tibble()

vare.dis <- vegdist(varespec)
vare.mds0 <- monoMDS(vare.dis)
```

#### 31 2b.Community.varespec **Question**

The `varespec` dataframe is 24 x 44. The rows are species and the columns are the estimated cover of the 44 species. 

#### 32 2b.Community.stressplot() 

```{r}
stressplot(vare.mds0)
```

#### 33 2b.Community.stressplot() **Question**

The non-metric fit has a R-squared value that is 0.1 higher than the linear fit which means that it is a better method. 

#### 34. 2b.Community.ordiplot()

```{r}

ordiplot(vare.mds0, type = "t")
```

#### 35 2b.Community.ordiplot() **Question**

For the first component, sites 4, 3, 2, and 5 are almost equally dissimilar with 28. For MDS2, sites 21 and 5 are the most dissimilar  

#### 36 2b.Community.metaMDS()

```{r}
vare.mds <- metaMDS(varespec, trace = F)
vare.mds
```

#### 37 2b.Community.metaMDS()

```{r}

plot(vare.mds, type = "t")
```


#### 38 2b.Community.metaMDS() **Question**

The basic difference is that `metaMDS()` performs several random starts using the functionality of monoMDS() but stops after meeting certain criteria. `monoMDS()` simply starts with the dissimilarities as input and finds two dimensions and uses random configuration to iterate but with no guaranteer convergence. 

### Overlay with Environment

#### 39 2b.Community.envfit() 

```{r}

ef <- envfit(vare.mds, varechem, permu = 999)
ef

plot(vare.mds, display = "sites")
plot(ef, p.max = 0.05)
```

#### 40 2b.Community.envfit() **Question** 

Aluminum and Iron have the strongest negative relationship with NMDS1 that is based on species composition. 


#### 41 2b.Community.rdisurf()

```{r}
ef <- envfit(vare.mds ~ Al + Ca, data = varechem)
plot(vare.mds, display = "sites")
plot(ef)

tmp <- with(varechem, ordisurf(vare.mds, Al, add = TRUE))
ordisurf(vare.mds ~ Ca, data=varechem, add = TRUE, col = "green4")

```

#### 42 2b.Community.ordisurf() **Question**

NMDS1 differentiates Calcium the most based on the values on the green contour lines. 

### Constrained Ordination on Species and Environment

```{r}

# this uses another technique cca, or canonical correspondence analysis.
# ordinate on species constrained by three soil elements
vare.cca <- cca(varespec ~ Al + P + K, varechem)
vare.cca
```

#### 43 2b.Community.cca() **Question** 

Constrained ordination explicitly includes two or more different sets of information into an analysis but unconstrained includes any potential effects by factors which were not included in an analysis. 

#### 44 2b.Community.cca()

```{r}

# plot ordination
plot(vare.cca)


# plot 3 dimensions
ordiplot3d(vare.cca, type = "h")


if (interactive()){
  ordirgl(vare.cca)
}

```

#### 45 2b.Community.cca() **Question**

Sites 4 and 28 are most differentiated by CCA1 based on species composition and the environment and the strongest environmental vector is Aluminum. 

